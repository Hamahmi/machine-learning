{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) Tutorial Using Iris Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting).\n",
    "Using [SVM](https://en.wikipedia.org/wiki/Support_vector_machine) on [Iris Dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) we will classify the flowers into 3 classes. \n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing All Libraries\n",
    "I prefere that all importings to be first but ofcourse you can import anywhere (This is Python, not as restrected as java :D )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing ✓\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries necessary for this project\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn                 import datasets\n",
    "from sklearn                 import preprocessing\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.preprocessing   import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm             import LinearSVC\n",
    "from sklearn.svm             import SVC\n",
    "\n",
    "print(\"\\nImporting ✓\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "Loading the data and getting information about it.\n",
    "\n",
    "Iris data sets has 3 types : \n",
    "- Setosa\n",
    "- Versicolour\n",
    "- Virginica\n",
    "\n",
    "And 4 features : \n",
    "- Sepal Length\n",
    "- Sepal Width\n",
    "- Petal Length\n",
    "- Petal Width.\n",
    "\n",
    "and consistes of 150 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading ✓\n",
      "\n",
      "Data size:  150\n",
      "Types    :  ['setosa' 'versicolor' 'virginica']\n",
      "Features :  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset\n",
    "\n",
    "iris     = datasets.load_iris()\n",
    "features = iris.data\n",
    "classes  = iris.target\n",
    "\n",
    "print(\"\\nLoading ✓\\n\")\n",
    "print(\"Data size: \", classes.size)\n",
    "print(\"Types    : \", (iris.target_names ))\n",
    "print(\"Features : \", (iris.feature_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing and Splitting the Data\n",
    "\n",
    "Normalizing the data is not very important but highly recomended to remove any bias that may affect the output.\n",
    "On the other hand splitting the data is very important so that we have 2 training sets and 2 testing sets with 80% of the data in the training set and the remaining 20% of the data in the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalizing ✓\n",
      "\n",
      "\n",
      "Splitting ✓\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the data\n",
    "scaler     = StandardScaler()\n",
    "normalizer = scaler.fit_transform(features, classes)\n",
    "\n",
    "print(\"\\nNormalizing ✓\\n\")\n",
    "\n",
    "# Splitting the data\n",
    "train_set_1, test_set_1, train_set_2, test_set_2 = train_test_split(normalizer, classes, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"\\nSplitting ✓\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a Model\n",
    "Using this [Cheat sheet](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) from [scikit-learn](http://scikit-learn.org/stable/index.html), one can directly chooses [Linear SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) and so did I."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC\n",
    "The data is only 150 which is small (in ML world) so using a Linear classifier we run grid search to get the best value of the C parameter.\n",
    "In the output one can see the score of running the algorithm using Linear Classifier on the training sets and on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best C is  :  490.7265473233038\n",
      "Training Score :  0.9666666666666667\n",
      "Testing Score  :  1.0\n",
      "\n",
      "Linear SVC ✓\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LSVC   = LinearSVC(class_weight=None, loss='hinge', multi_class='ovr', random_state=0)\n",
    "\n",
    "# TODO : Change the range to see the effect on the output ( best result I got (\n",
    "# The best C is  :  490.7265473233038\n",
    "# Training Score :  0.9666666666666667\n",
    "# Testing Score  :  1.0))\n",
    "\n",
    "CRange = {'C': np.logspace(10^-7, 10^20, base=2)}\n",
    "GSVC   = GridSearchCV(LSVC, CRange, cv=5)\n",
    "\n",
    "GSVC.fit(train_set_1, train_set_2)\n",
    "\n",
    "best_E = GSVC.best_estimator_\n",
    "\n",
    "print(\"The best C is  : \", best_E.C)\n",
    "print(\"Training Score : \", best_E.score(train_set_1, train_set_2))\n",
    "print(\"Testing Score  : \", best_E.score(test_set_1, test_set_2))\n",
    "print(\"\\nLinear SVC ✓\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel\n",
    "\n",
    "To improve our model's accuracy we use [kernels](https://en.wikipedia.org/wiki/Kernel_method). \n",
    "In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. \n",
    "There are several different types of kernels such as gaussian rbf and polynomial poly kernels. \n",
    "In this tutorial we will use Polynomial kernel (but you can use another kernel if you want try in the code block below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best C is  :  17.665432218780992\n",
      "Training Score :  0.9833333333333333\n",
      "Testing Score  :  1.0\n",
      "\n",
      "Polynomial SVC ✓\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO change the parameters to see the change in the output \n",
    "parameters = {'C': np.logspace(-10, 11, base=2),'degree': np.arange(2,3),'coef0': np.arange(2,3)}\n",
    "        \n",
    "GSVC1 = GridSearchCV(SVC(kernel='poly', gamma='auto', random_state=0), parameters, cv=5 )\n",
    "\n",
    "GSVC1.fit(train_set_1, train_set_2)\n",
    "\n",
    "best_E = GSVC1.best_estimator_\n",
    "\n",
    "print(\"The best C is  : \", best_E.C)\n",
    "print(\"Training Score : \", best_E.score(train_set_1, train_set_2))\n",
    "print(\"Testing Score  : \", best_E.score(test_set_1, test_set_2))\n",
    "print(\"\\nPolynomial SVC ✓\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see the training score improves and the test score is 100% (which may be called overfitting but the dataset is small as mentioned above)\n",
    "That concludes our tutorial, to know more about SVC you can visit these sites :\n",
    "- [Week 7 in this course](https://www.coursera.org/lecture/machine-learning/optimization-objective-sHfVT)\n",
    "- [Tutorials from scikit-learn](http://scikit-learn.org/stable/index.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
